# MIET.AI.Course

# План курса лекций «компьютерное зрение»

## Лекция 0. Python

1. Вводная беседа
2. Основы python

Источник: https://docs.python.org/3/tutorial/

## Лекция 1. Анализ табличных данных

1. skitit-learn
2. xgbboost
3. Сравнение линейной регрессии и xgbboost на конкретном примере обработки данных

Источники:

[https://github.com/dmlc/xgboost/tree/master/demo#machine-learning-challenge-winning-solutions](https://github.com/dmlc/xgboost/tree/master/demo#machine-learning-challenge-winning-solutions)

[https://machinelearningmastery.com/gentle-introduction-xgboost-applied-machine-learning/](https://machinelearningmastery.com/gentle-introduction-xgboost-applied-machine-learning/)

[https](https://habr.com/en/company/ods/blog/327250/)[://](https://habr.com/en/company/ods/blog/327250/)[habr](https://habr.com/en/company/ods/blog/327250/)[.](https://habr.com/en/company/ods/blog/327250/)[com](https://habr.com/en/company/ods/blog/327250/)[/](https://habr.com/en/company/ods/blog/327250/)[en](https://habr.com/en/company/ods/blog/327250/)[/](https://habr.com/en/company/ods/blog/327250/)[company](https://habr.com/en/company/ods/blog/327250/)[/](https://habr.com/en/company/ods/blog/327250/)[ods](https://habr.com/en/company/ods/blog/327250/)[/](https://habr.com/en/company/ods/blog/327250/)[blog](https://habr.com/en/company/ods/blog/327250/)[/327250/](https://habr.com/en/company/ods/blog/327250/)

[https://dl.acm.org/doi/pdf/10.1145/2939672.2939785?download=true](https://dl.acm.org/doi/pdf/10.1145/2939672.2939785?download=true)

## Лекция 2. Свёрточные нейронные сети и классификация изображений

1. Вводная часть про обучение нейронных сетей, какие проблемы приходится решать
2. MNIST и LeNet
3. Задача ImageNet

Источники:

[https](https://arxiv.org/pdf/1609.04747.pdf)[://](https://arxiv.org/pdf/1609.04747.pdf)[arxiv](https://arxiv.org/pdf/1609.04747.pdf)[.](https://arxiv.org/pdf/1609.04747.pdf)[org](https://arxiv.org/pdf/1609.04747.pdf)[/](https://arxiv.org/pdf/1609.04747.pdf)[pdf](https://arxiv.org/pdf/1609.04747.pdf)[/1609.04747.](https://arxiv.org/pdf/1609.04747.pdf)[pdf](https://arxiv.org/pdf/1609.04747.pdf)

[https](https://www.eecis.udel.edu/~shatkay/Course/papers/NetworksAndCNNClasifiersIntroVapnik95.pdf)[://](https://www.eecis.udel.edu/~shatkay/Course/papers/NetworksAndCNNClasifiersIntroVapnik95.pdf)[www](https://www.eecis.udel.edu/~shatkay/Course/papers/NetworksAndCNNClasifiersIntroVapnik95.pdf)[.](https://www.eecis.udel.edu/~shatkay/Course/papers/NetworksAndCNNClasifiersIntroVapnik95.pdf)[eecis](https://www.eecis.udel.edu/~shatkay/Course/papers/NetworksAndCNNClasifiersIntroVapnik95.pdf)[.](https://www.eecis.udel.edu/~shatkay/Course/papers/NetworksAndCNNClasifiersIntroVapnik95.pdf)[udel](https://www.eecis.udel.edu/~shatkay/Course/papers/NetworksAndCNNClasifiersIntroVapnik95.pdf)[.](https://www.eecis.udel.edu/~shatkay/Course/papers/NetworksAndCNNClasifiersIntroVapnik95.pdf)[edu](https://www.eecis.udel.edu/~shatkay/Course/papers/NetworksAndCNNClasifiersIntroVapnik95.pdf)[/~](https://www.eecis.udel.edu/~shatkay/Course/papers/NetworksAndCNNClasifiersIntroVapnik95.pdf)[shatkay](https://www.eecis.udel.edu/~shatkay/Course/papers/NetworksAndCNNClasifiersIntroVapnik95.pdf)[/](https://www.eecis.udel.edu/~shatkay/Course/papers/NetworksAndCNNClasifiersIntroVapnik95.pdf)[Course](https://www.eecis.udel.edu/~shatkay/Course/papers/NetworksAndCNNClasifiersIntroVapnik95.pdf)[/](https://www.eecis.udel.edu/~shatkay/Course/papers/NetworksAndCNNClasifiersIntroVapnik95.pdf)[papers](https://www.eecis.udel.edu/~shatkay/Course/papers/NetworksAndCNNClasifiersIntroVapnik95.pdf)[/](https://www.eecis.udel.edu/~shatkay/Course/papers/NetworksAndCNNClasifiersIntroVapnik95.pdf)[NetworksAndCNNClasifiersIntroVapnik](https://www.eecis.udel.edu/~shatkay/Course/papers/NetworksAndCNNClasifiersIntroVapnik95.pdf)[95.](https://www.eecis.udel.edu/~shatkay/Course/papers/NetworksAndCNNClasifiersIntroVapnik95.pdf)[pdf](https://www.eecis.udel.edu/~shatkay/Course/papers/NetworksAndCNNClasifiersIntroVapnik95.pdf)

[https://arxiv.org/pdf/1502.03167.pdf](https://arxiv.org/pdf/1502.03167.pdf)

[http](http://www.vlfeat.org/matconvnet/matconvnet-manual.pdf)[://](http://www.vlfeat.org/matconvnet/matconvnet-manual.pdf)[www](http://www.vlfeat.org/matconvnet/matconvnet-manual.pdf)[.](http://www.vlfeat.org/matconvnet/matconvnet-manual.pdf)[vlfeat](http://www.vlfeat.org/matconvnet/matconvnet-manual.pdf)[.](http://www.vlfeat.org/matconvnet/matconvnet-manual.pdf)[org](http://www.vlfeat.org/matconvnet/matconvnet-manual.pdf)[/](http://www.vlfeat.org/matconvnet/matconvnet-manual.pdf)[matconvnet](http://www.vlfeat.org/matconvnet/matconvnet-manual.pdf)[/](http://www.vlfeat.org/matconvnet/matconvnet-manual.pdf)[matconvnet](http://www.vlfeat.org/matconvnet/matconvnet-manual.pdf)[-](http://www.vlfeat.org/matconvnet/matconvnet-manual.pdf)[manual](http://www.vlfeat.org/matconvnet/matconvnet-manual.pdf)[.](http://www.vlfeat.org/matconvnet/matconvnet-manual.pdf)[pdf](http://www.vlfeat.org/matconvnet/matconvnet-manual.pdf)

[http://www.image-net.org](http://www.image-net.org)

Николенко и др., Глубокое обучение

Goodfellow

## Лекция 3. Нейросетевые детекторы положения объектов на изображении

1. Region proposals via selective search R-CNN
2. Fast R-CNN
3. Faster R-CNN
4. YOLO, SSD

[https://towardsdatascience.com/r-cnn-fast-r-cnn-faster-r-cnn-yolo-object-detection-algorithms-36d53571365e](https://towardsdatascience.com/r-cnn-fast-r-cnn-faster-r-cnn-yolo-object-detection-algorithms-36d53571365e)

[http://openaccess.thecvf.com/content\_iccv\_2015/papers/Girshick\_Fast\_R-CNN\_ICCV\_2015\_paper.pdf](http://openaccess.thecvf.com/content_iccv_2015/papers/Girshick_Fast_R-CNN_ICCV_2015_paper.pdf)

[http://papers.nips.cc/paper/5638-faster-r-cnn-towards-real-time-object-detection-with-region-proposal-networks](http://papers.nips.cc/paper/5638-faster-r-cnn-towards-real-time-object-detection-with-region-proposal-networks)

[http://openaccess.thecvf.com/content\_cvpr\_2017/html/Redmon\_YOLO9000\_Better\_Faster\_CVPR\_2017\_paper.html](http://openaccess.thecvf.com/content_cvpr_2017/html/Redmon_YOLO9000_Better_Faster_CVPR_2017_paper.html)

[https://arxiv.org/pdf/1512.02325.pdf](https://arxiv.org/pdf/1512.02325.pdf)

## Лекция 4. Нейросетевые методы поиска особых точек OpenPose

1. [Shotton, Jamie, Ross Girshick, Andrew Fitzgibbon, Toby Sharp, Mat Cook, Mark Finocchio, Richard Moore et al. "Efficient human pose estimation from single depth images." IEEE transactions on pattern analysis and machine intelligence 35, no. 12 (2012): 2821-2840.](https://www.microsoft.com/en-us/research/wp-content/uploads/2016/02/main-39.pdf)

2. [Tompson, Jonathan, Ross Goroshin, Arjun Jain, Yann LeCun, and Christoph Bregler. "Efficient object localization using convolutional networks." In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 648-656. 2015.](https://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Tompson_Efficient_Object_Localization_2015_CVPR_paper.pdf)

3. [Ramakrishna, Varun, Daniel Munoz, Martial Hebert, James Andrew Bagnell, and Yaser Sheikh. "Pose machines: Articulated pose estimation via inference machines." In European Conference on Computer Vision, pp. 33-47. Springer, Cham, 2014.](https://kilthub.cmu.edu/articles/Pose_Machines_Articulated_Pose_Estimation_via_Inference_Machines/6558671/files/12040949.pdf)

4. [Cao, Zhe, Gines Hidalgo, Tomas Simon, Shih-En Wei, and Yaser Sheikh. "OpenPose: realtime multi-person 2D pose estimation using Part Affinity Fields." arXiv preprint arXiv:1812.08008 (2018).](https://arxiv.org/pdf/1812.08008.pdf)

5. [Sun, Ke, Bin Xiao, Dong Liu, and Jingdong Wang. "Deep high-resolution representation learning for human pose estimation." In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 5693-5703. 2019.](https://github.com/microsoft/human-pose-estimation.pytorch)

## Лекция 5. GANs

1. [Gui, Jie, Zhenan Sun, Yonggang Wen, Dacheng Tao, and Jieping Ye. "A Review on Generative Adversarial Networks: Algorithms, Theory, and Applications." arXiv preprint arXiv:2001.06937 (2020).](https://arxiv.org/pdf/2001.06937.pdf)

2. [Kingma, Diederik P., and Max Welling. "Auto-encoding variational bayes." arXiv preprint arXiv:1312.6114 (2013).](https://arxiv.org/pdf/1312.6114.pdf)

3. [Pu, Yunchen, Zhe Gan, Ricardo Henao, Xin Yuan, Chunyuan Li, Andrew Stevens, and Lawrence Carin. "Variational autoencoder for deep learning of images, labels and captions." In Advances in neural information processing systems, pp. 2352-2360. 2016.](http://papers.nips.cc/paper/6528-variational-autoencoder-for-deep-learning-of-images-labels-and-captions.pdf)

4. [Makhzani, Alireza, Jonathon Shlens, Navdeep Jaitly, Ian Goodfellow, and Brendan Frey. "Adversarial autoencoders." arXiv preprint arXiv:1511.05644 (2015).](https://arxiv.org/pdf/1511.05644.pdf)

5. [Goodfellow, Ian, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. "Generative adversarial nets." In Advances in neural information processing systems, pp. 2672-2680. 2014.](http://papers.nips.cc/paper/5423-generative-adversarial-nets.pdf)

6. [Chen, Xi, Yan Duan, Rein Houthooft, John Schulman, Ilya Sutskever, and Pieter Abbeel. "Infogan: Interpretable representation learning by information maximizing generative adversarial nets." In Advances in neural information processing systems, pp. 2172-2180. 2016.](http://papers.nips.cc/paper/6399-infogan-interpretable-representation-learning-by-information-maximizing-generative-adversarial-nets.pdf)

7. [Reed, Scott E., Zeynep Akata, Santosh Mohan, Samuel Tenka, Bernt Schiele, and Honglak Lee. "Learning what and where to draw." In Advances in neural information processing systems, pp. 217-225. 2016.](https://arxiv.org/pdf/1610.02454.pdf)



8. [Isola, Phillip, Jun-Yan Zhu, Tinghui Zhou, and Alexei A. Efros. "Image-to-image translation with conditional adversarial networks." In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 1125-1134. 2017.](https://arxiv.org/pdf/1611.07004.pdf)

9. [Karras, Tero, Samuli Laine, and Timo Aila. "A style-based generator architecture for generative adversarial networks." In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 4401-4410. 2019.](http://openaccess.thecvf.com/content_CVPR_2019/papers/Karras_A_Style-Based_Generator_Architecture_for_Generative_Adversarial_Networks_CVPR_2019_paper.pdf)



## Лекция 6. Способы подготовки данных для обучения нейронных сетей

1. Доверительный интервал для оценки достоверности классификации
2. Оценки объёмов тестирующих выборок
3. Источники данных
4. Платформы mturk, toloka
5. Симуляционные данные
6. Трюки при обучении (pseudo-labeling, аугментация)

Источники:

https://ru.coursera.org/lecture/stats-for-data-analysis/dovieritiel-nyie-intiervaly-s-pomoshch-iu-kvantiliei-yboDc 

https://sebastianraschka.com/blog/2018/model-evaluation-selection-part4.html

https://www.mturk.com

https://toloka.yandex.ru/tasks

https://github.com/immersive-limit/Unity-ComputerVisionSim

## Лекция 7. Методы ускорения нейросетевых вычислений

1. [Пример кода с использованием SIMD-инструкций](https://gist.github.com/vermorel/7ad35212df44f3a79bca8ab5fe8e7622#file-fast_convolve_1d-cpp-L123)
2. [Библиотека Openvino](https://software.intel.com/en-us/openvino-toolkit/choose-download)
3. [Howard, Andrew G., Menglong Zhu, Bo Chen, Dmitry Kalenichenko, Weijun Wang, Tobias Weyand, Marco Andreetto, and Hartwig Adam. "Mobilenets: Efficient convolutional neural networks for mobile vision applications." arXiv preprint arXiv:1704.04861 (2017).](https://arxiv.org/pdf/1704.04861.pdf)
4. [Sandler, Mark, Andrew Howard, Menglong Zhu, Andrey Zhmoginov, and Liang-Chieh Chen. "Mobilenetv2: Inverted residuals and linear bottlenecks." In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 4510-4520. 2018.](https://arxiv.org/pdf/1801.04381.pdf)
5. [Courbariaux, Matthieu, Itay Hubara, Daniel Soudry, Ran El-Yaniv, and Yoshua Bengio. "Binarized neural networks: Training deep neural networks with weights and activations constrained to+ 1 or-1." arXiv preprint arXiv:1602.02830(2016).](https://arxiv.org/pdf/1602.02830)
6. [Rastegari, Mohammad, Vicente Ordonez, Joseph Redmon, and Ali Farhadi. "Xnor-net: Imagenet classification using binary convolutional neural networks." In European conference on computer vision, pp. 525-542. Springer, Cham, 2016.](https://arxiv.org/pdf/1603.05279.pdf)
7. [BMXNet: An Open-Source Binary Neural Network Implementation Based on MXNet](https://github.com/hpi-xnor/BMXNet)


## Лекция 8. Классические методы компьютерного зрения: вычитание фона

1. [Collins, Robert T., Alan J. Lipton, Takeo Kanade, Hironobu Fujiyoshi, David Duggins, Yanghai Tsin, David Tolliver et al. "A system for video surveillance and monitoring." VSAM final report 2000 (2000): 1-68.](http://ri.cmu.edu/pub_files/pub2/collins_robert_2000_1/collins_robert_2000_1.pdf)

2. [Stauffer, Chris, and W. Eric L. Grimson. "Adaptive background mixture models for real-time tracking." In Proceedings. 1999 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (Cat. No PR00149), vol. 2, pp. 246-252. IEEE, 1999.](http://mesh.brown.edu/en193s05-2004/pdfs/StaufferGrimson-cvpr98.pdf)

3. [Goyette, Nil, Pierre-Marc Jodoin, Fatih Porikli, Janusz Konrad, and Prakash Ishwar. "Changedetection. net: A new change detection benchmark dataset." In 2012 IEEE computer society conference on computer vision and pattern recognition workshops, pp. 1-8. IEEE, 2012.](https://www.merl.com/publications/docs/TR2012-044.pdf)

4. [Van Droogenbroeck, Marc, and Olivier Paquot. "Background subtraction: Experiments and improvements for ViBe." In 2012 IEEE computer society conference on computer vision and pattern recognition workshops, pp. 32-37. IEEE, 2012.](https://www.researchgate.net/profile/Marc_Droogenbroeck/publication/252067585_Background_Subtraction_Experiments_and_Improvements_for_ViBe/links/553a5bce0cf29b5ee4b626d0/Background-Subtraction-Experiments-and-Improvements-for-ViBe.pdf)

5. [Hofmann, Martin, Philipp Tiefenbacher, and Gerhard Rigoll. "Background segmentation with feedback: The pixel-based adaptive segmenter." In 2012 IEEE computer society conference on computer vision and pattern recognition workshops, pp. 38-43. IEEE, 2012.](https://mediatum.ub.tum.de/doc/1137859/file.pdf)

6. [Wang, Rui, Filiz Bunyak, Guna Seetharaman, and Kannappan Palaniappan. "Static and moving object detection using flux tensor with split gaussian models." In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops, pp. 414-418. 2014.](https://www.cv-foundation.org/openaccess/content_cvpr_workshops_2014/W12/papers/Wang_Static_and_Moving_2014_CVPR_paper.pdf)

7. [Lim, Long Ang, and Hacer Yalim Keles. "Learning multi-scale features for foreground segmentation." Pattern Analysis and Applications (2019): 1-12.](https://arxiv.org/pdf/1808.01477)

8. [Я.Я. Петричкович, А.В. Хамухин. Анализ влияния метода вычитания фона на конечную эффективность систем компьютерного зрения.](https://github.com/anakham/MIET.AI.Course/blob/master/Лекция%208/impact_of_bg_substraction.pdf)

## Лекция 9. Классические методы компьютерного зрения: вычисление точек особенностей. Усиление метода нейронными сетями

1. [Harris, Christopher G., and Mike Stephens. "A combined corner and edge detector." Alvey vision conference. Vol. 15. No. 50. 1988.](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.434.4816&rep=rep1&type=pdf)

2. [Derpanis, Konstantinos G. "The harris corner detector." York University 2 (2004).](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.482.1724&rep=rep1&type=pdf)

3. [Lowe, David G. "Distinctive image features from scale-invariant keypoints." International journal of computer vision 60.2 (2004): 91-110.](https://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.313.1996&rep=rep1&type=pdf)

4. [Lindeberg, Tony. "Feature detection with automatic scale selection." International journal of computer vision 30.2 (1998): 79-116.](https://www.diva-portal.org/smash/get/diva2:453064/FULLTEXT01.pdf)

5. [Rublee, Ethan, et al. "ORB: An efficient alternative to SIFT or SURF." 2011 International conference on computer vision. Ieee, 2011.](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.370.4395&rep=rep1&type=pdf)

6. [Rosten, Edward, and Tom Drummond. "Machine learning for high-speed corner detection." European conference on computer vision. Springer, Berlin, Heidelberg, 2006.](https://link.springer.com/content/pdf/10.1007/11744023_34.pdf)

7. [Calonder, Michael, et al. "BRIEF: Computing a local binary descriptor very fast." IEEE transactions on pattern analysis and machine intelligence 34.7 (2011): 1281-1298.](https://infoscience.epfl.ch/record/167678/files/top.pdf)

8. [DeTone, Daniel, Tomasz Malisiewicz, and Andrew Rabinovich. "Superpoint: Self-supervised interest point detection and description." Proceedings of the IEEE conference on computer vision and pattern recognition workshops. 2018.](https://openaccess.thecvf.com/content_cvpr_2018_workshops/papers/w9/DeTone_SuperPoint_Self-Supervised_Interest_CVPR_2018_paper.pdf)

8. [Barroso-Laguna, Axel, et al. "Key. net: Keypoint detection by handcrafted and learned cnn filters." Proceedings of the IEEE/CVF International Conference on Computer Vision. 2019.](http://openaccess.thecvf.com/content_ICCV_2019/papers/Barroso-Laguna_Key.Net_Keypoint_Detection_by_Handcrafted_and_Learned_CNN_Filters_ICCV_2019_paper.pdf)



## Лекция 10. Обобщённые дескрипторы изображений, tripletloss.

## Лекция 11. Реккурентные нейронные сети в компьютерном зрении. GRU, LSTM, visual question answering

## Лекция 12. Обучение с подкреплением
